{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the UCSD course catalog\n",
    "The UCSD course catalog contains 6,000+ course descriptions across 80+ pages. The HTML is messy, inconsistent, and full of surprises, so this was lots of fun."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I get the links to the individual program pages (e.g. CSE, LANG, MATH) from the \"cover\" of the course catalog. I just get every link named \"courses\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "page = requests.get('https://catalog.ucsd.edu/front/courses.html')\n",
    "cover = BeautifulSoup(page.text, 'html.parser')\n",
    "links = cover.find_all('a', href=lambda href: href and href.startswith('../courses'))\n",
    "urls = ['https://catalog.ucsd.edu' + link['href'][2:] for link in links]\n",
    "\n",
    "def get_dept(url):\n",
    "    return url[33:-5].upper()\n",
    "\n",
    "with open('urls.txt', 'w') as f:\n",
    "    for url in urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all the pages (takes like 20-30 seconds)\n",
    "pages = [BeautifulSoup(requests.get(url).content, 'html.parser') for url in urls]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the code below to show me all the most uncommon combinations of tag names and classes. It helped me find all sorts of oddities and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key(tag):\n",
    "    return (tag.name, tuple(tag.attrs.get('class') or []))\n",
    "\n",
    "tag_types = {}\n",
    "\n",
    "with open('edge-cases.txt', 'w') as f:\n",
    "    for url, page in zip(urls, pages):\n",
    "        # f.write('\\n\\n==========================================\\n\\n' + url + '\\n\\n==========================================\\n\\n')\n",
    "        content = page.find(class_=\"col-md-12 blank-slate\").children\n",
    "        for tag in content:\n",
    "            if not isinstance(tag, bs4.element.Tag):\n",
    "                continue\n",
    "            \n",
    "            k = key(tag)\n",
    "            if k not in tag_types:\n",
    "                tag_types[k] = [0, set()]\n",
    "            tag_types[k][0] += 1\n",
    "            tag_types[k][1].add(url)\n",
    "\n",
    "    data = [[v[0], k[0], ' '.join(k[1]), ' '.join(v[1])] for k, v in tag_types.items()]\n",
    "    data.sort()\n",
    "\n",
    "    f.write(tabulate(data, headers = [\"Count\", \"Tag\", \"Classes\", \"URLs\"]))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML is messy, but at least every page is structured similarly: all of the info I care about is in a `div` with class `\"col-md-12 blank-slate\"`. After that, I go off of the tag names and classes of that `div`'s direct children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes i definitely dont care about\n",
    "ignored_classes = set([\n",
    "    'course-head',\n",
    "    'basic-offset-top-only',\n",
    "    'faculty-staff-subhead',\n",
    "    'note',\n",
    "    'course-note',\n",
    "    'alphabreak',\n",
    "    'sectionNav', \n",
    "    'program-contact-info',\n",
    "    'anchor-parent',\n",
    "    'courseFacLink',\n",
    "    'course-list-overview',\n",
    "    'course-prerequisite-paragraph',\n",
    "    'course-list-courses',\n",
    "    'course-disclaimer',\n",
    "])\n",
    "\n",
    "# tag names i definitely dont care about\n",
    "ignored_tag_names = set([\n",
    "    'ul', 'table', 'a'\n",
    "])\n",
    "\n",
    "def get_class(tag: bs4.element.Tag) -> str:\n",
    "    classes = tag.attrs.get('class')\n",
    "    if not classes: return ''\n",
    "    if len(classes) > 1:\n",
    "        raise Exception('multiple classes')\n",
    "\n",
    "    # all instances of 'course-description' are typos of 'course-descriptions'\n",
    "    # just shorten both to desc here\n",
    "    # (not that it even matters because I'm ignoring them both)\n",
    "    if classes[0] in ('course-description', 'course-descriptions'):\n",
    "        return 'desc'\n",
    "    \n",
    "    return classes[0]\n",
    "\n",
    "def ignored(tag):\n",
    "    return get_class(tag) in ignored_classes or tag.name in ignored_tag_names\n",
    "\n",
    "def get_tags(page: BeautifulSoup) -> list[bs4.element.Tag]:\n",
    "    tags = page.find(class_=\"col-md-12 blank-slate\").children\n",
    "    return [tag for tag in tags if type(tag) is bs4.element.Tag and not ignored(tag)] # tag is bs4.element.Tag and"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I go through the tags I might care about (`get_tags(page)`) and extract the just the courses & categories for now - we'll process each course more afterwards. The most important thing I rely on is that each course title is a paragraph with class \"course-name\". I use the headers to give the courses categories. I write all the content into a file in the form of JSON so I can manually fix typos/formatting and add/remove content. My JSON will be a list of the 82 pages, each of which is a list of courses (title and description) and headers (h2-h4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(page: BeautifulSoup):\n",
    "    def is_header(tag: bs4.element.Tag):\n",
    "        return tag.name[0] == 'h'\n",
    "    \n",
    "    def is_title(tag: bs4.element.Tag):\n",
    "        return get_class(tag) == 'course-name'\n",
    "\n",
    "    seen_courses_header = False\n",
    "    started = False\n",
    "\n",
    "    # instead of relying on any other classes/tags, just add all text to the course\n",
    "    # description unless it is between an h tag and a <p class=\"course-name\">\n",
    "    adding_text = False\n",
    "\n",
    "    ret = [] # list of courses and headers in JSON format\n",
    "    for tag in get_tags(page):\n",
    "        if not started:\n",
    "            # start at either at the first course-name or the first header AFTER \"Courses\"\n",
    "            if tag.text == 'Courses':\n",
    "                seen_courses_header = True\n",
    "                continue\n",
    "            elif is_title(tag):\n",
    "                started = True\n",
    "            elif is_header(tag) and seen_courses_header:\n",
    "                started = True\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if is_header(tag):\n",
    "            ret.append({\n",
    "                'type': tag.name,\n",
    "                'content': tag.text\n",
    "            })\n",
    "            adding_text = False # don't add text between headers and course-names\n",
    "        elif is_title(tag):\n",
    "            adding_text = True\n",
    "            ret.append({\n",
    "                'type': 'course',\n",
    "                'title': tag.text,\n",
    "                'desc': ''\n",
    "            })\n",
    "        elif adding_text:\n",
    "            if ret[-1]['desc'] != '':\n",
    "                ret[-1]['desc'] += '\\n'\n",
    "            ret[-1]['desc'] += tag.text\n",
    "    \n",
    "    return ret\n",
    "\n",
    "pages_data = [{\n",
    "    'url': url,\n",
    "    'dept': get_dept(url),\n",
    "    'content': get_clean_data(page)\n",
    "} for url, page in zip(urls, pages)]\n",
    "\n",
    "with open('pages-data.json', 'w') as f:\n",
    "    json.dump(pages_data, f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I get the subject codes (e.g. CSE, LANG) and the major codes (e.g. CS27, UNHA) from their respective pages. I don't worry about minor codes because the catalog doesn't really use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major codes\n",
    "soup = BeautifulSoup(requests.get('https://blink.ucsd.edu/instructors/academic-info/majors/major-codes.html').content, 'html.parser')\n",
    "major_codes = {}\n",
    "for row in soup.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if not cells or len(cells) < 2 or len(cells[-2].text) < 3:\n",
    "        continue\n",
    "    try:\n",
    "        major_codes[cells[-2].text] = cells[-1].text\n",
    "    except IndexError:\n",
    "        print(cells)\n",
    "\n",
    "\n",
    "# subject codes\n",
    "soup = BeautifulSoup(requests.get('https://blink.ucsd.edu/instructors/courses/schedule-of-classes/subject-codes.html').content, 'html.parser')\n",
    "subject_codes = {}\n",
    "for row in soup.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if not cells or len(cells) < 2:\n",
    "        continue\n",
    "    subject_codes[cells[0].text] = cells[1].text\n",
    "\n",
    "# print(tabulate([[k, v] for k, v in major_codes.items()], headers=['Code', 'Major']))\n",
    "# print(tabulate([[k, v] for k, v in subject_codes.items()], headers=['Subject', 'Major']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
